{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterData(row, dados_csv, data_hora_ini, data_hora_fim, insert_queimada, insert_queimada_comp, isRange):\n",
    "    data_hora_csv_str = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}', row['data_hora_gmt'])\n",
    "    data_hora_csv = datetime.strptime(data_hora_csv_str[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "    if(data_hora_ini <= data_hora_csv <= data_hora_fim):\n",
    "                \n",
    "        foreign_key = insertTable(row)                                        # database\n",
    "        createInsert(row, insert_queimada, insert_queimada_comp, foreign_key) # script\n",
    "                    \n",
    "        dados_csv = np.append(dados_csv, row)\n",
    "\n",
    "        isRange = True\n",
    "    if (data_hora_csv > data_hora_fim):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateToAcronym(estado):\n",
    "  match(estado):\n",
    "      case 'ACRE':\n",
    "        estado = 'AC'\n",
    "\n",
    "      case 'ALAGOAS':\n",
    "        estado = 'AL'\n",
    "\n",
    "      case 'AMAPÁ':\n",
    "        estado = 'AP'\n",
    "\n",
    "      case 'AMAZONAS':\n",
    "        estado = 'AM'\n",
    "\n",
    "      case 'BAHIA':\n",
    "        estado = 'BA'\n",
    "\n",
    "      case 'CEARÁ':\n",
    "        estado = 'CE'\n",
    "\n",
    "      case 'ESPÍRITO SANTO':\n",
    "        estado = 'ES'\n",
    "\n",
    "      case 'GOIÁS':\n",
    "        estado = 'GO'\n",
    "\n",
    "      case 'MARANHÃO':\n",
    "        estado = 'MA'\n",
    "\n",
    "      case 'MATO GROSSO':\n",
    "        estado = 'MT'\n",
    "\n",
    "      case 'MATO GROSSO DO SUL':\n",
    "        estado = 'MS'\n",
    "\n",
    "      case 'MINAS GERAIS':\n",
    "        estado = 'MG'\n",
    "\n",
    "      case 'PARÁ':\n",
    "        estado = 'PA'\n",
    "\n",
    "      case 'PARAÍBA':\n",
    "        estado = 'PB'\n",
    "\n",
    "      case 'PARANÁ':\n",
    "        estado = 'PR'\n",
    "\n",
    "      case 'PERNAMBUCO':\n",
    "        estado = 'PE'\n",
    "\n",
    "      case 'PIAUÍ':\n",
    "        estado = 'PI'\n",
    "\n",
    "      case 'RIO DE JANEIRO':\n",
    "        estado = 'RJ'\n",
    "\n",
    "      case 'RIO GRANDE DO NORTE':\n",
    "        estado = 'RN'\n",
    "\n",
    "      case 'RIO GRANDE DO SUL':\n",
    "        estado = 'RS'\n",
    "\n",
    "      case 'RONDÔNIA':\n",
    "        estado = 'RO'\n",
    "\n",
    "      case 'RORAIMA':\n",
    "        estado = 'RR'\n",
    "\n",
    "      case 'SANTA CATARINA':\n",
    "        estado = 'SC'\n",
    "\n",
    "      case 'SÃO PAULO':\n",
    "        estado = 'SP'\n",
    "\n",
    "      case 'SERGIPE':\n",
    "        estado = 'SE'\n",
    "\n",
    "      case 'TOCANTINS':\n",
    "        estado = 'TO'\n",
    "\n",
    "      case 'DISTRITO FEDERAL':\n",
    "        estado = 'DF'\n",
    "  return estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInsert(row, insert_queimada, insert_queimada_comp, foreign_key):\n",
    "    latitude  = row['lat'].strip()\n",
    "    longitude = row['lon'].strip()\n",
    "    municipio = ftfy.fix_text(row['municipio'].replace(\"'\",\"''\"))\n",
    "    \n",
    "    estado = stateToAcronym(ftfy.fix_text(row['estado']))\n",
    "        \n",
    "    insert_queimada.append(f\"INSERT INTO QUEIMADA (DataHora, Latitude, Longitude, Estado, Municipio, Satelite) VALUES ('{row['data_hora_gmt']}', '{latitude}', '{longitude}', '{estado}', '{municipio}', '{row['satelite']}');\\n\")\n",
    "    insert_queimada_comp.append(f\"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES ('{foreign_key}', 'Bioma', '{row['bioma']}');\\n\")\n",
    "    insert_queimada_comp.append(f\"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES ('{foreign_key}', 'Diasemchuva', '{row['numero_dias_sem_chuva']}');\\n\")\n",
    "    insert_queimada_comp.append(f\"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES ('{foreign_key}', 'Precipitacao', '{row['precipitacao']}');\\n\")\n",
    "    insert_queimada_comp.append(f\"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES ('{foreign_key}', 'Riscofogo', '{row['risco_fogo']}');\\n\")    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "def connection_database():\n",
    "    server = \"DJ10W9D3\\SQLEXPRESS\"\n",
    "    database = \"bd_sinape_alupar\"\n",
    "    username = \"sa\"\n",
    "    password = \"DAS@sinap\"\n",
    "    \n",
    "    connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};sslverify=0'\n",
    "    return connection_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertTable(row):\n",
    "    id = 0\n",
    "    try:\n",
    "        connection = pyodbc.connect(connection_database())\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        cursor.execute('SELECT MAX(COD_QUEIM) FROM QUEIMADA')\n",
    "        result_set = cursor.fetchall()\n",
    "    \n",
    "        if result_set and result_set[0][0] is not None:\n",
    "            id = result_set[0][0] + 1\n",
    "        else:\n",
    "            id = 1\n",
    "\n",
    "    \n",
    "        latitude  = row['lat'].strip()\n",
    "        longitude = row['lon'].strip()\n",
    "        municipio = ftfy.fix_text(row['municipio'].replace(\"'\",\"''\"))\n",
    "        estado    = stateToAcronym(ftfy.fix_text(row['estado']))\n",
    "        satelite  = ftfy.fix_text(row['satelite'])\n",
    "        \n",
    "        bioma        = ftfy.fix_text(row['bioma'])\n",
    "        chuva        = ftfy.fix_text(row['numero_dias_sem_chuva'])\n",
    "        precipitacao = ftfy.fix_text(row['precipitacao'])\n",
    "        fogo         = ftfy.fix_text(row['risco_fogo'])\n",
    "        \n",
    "        \n",
    "        sql_insert_queimada = \"INSERT INTO QUEIMADA (DataHora, Latitude, Longitude, Estado, Municipio, Satelite) VALUES (?, ?, ?, ?, ?, ?);\"\n",
    "        values_queimada = (row['data_hora_gmt'], latitude, longitude, estado, municipio, satelite)\n",
    "        \n",
    "        sql_insert_queimada_complemento_bioma = \"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES (?, ?, ?)\"\n",
    "        values_complemento_bioma = (id, 'Bioma', bioma)\n",
    "        \n",
    "        sql_insert_queimada_complemento_chuva = \"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES (?, ?, ?)\"\n",
    "        values_complemento_chuva = (id, 'Diasemchuva', chuva)\n",
    "        \n",
    "        sql_insert_queimada_complemento_precipitacao = \"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES (?, ?, ?)\"\n",
    "        values_complemento_precipitacao = (id, 'Precipitacao', precipitacao)\n",
    "        \n",
    "        sql_insert_queimada_complemento_fogo = \"INSERT INTO QUEIMADA_COMPLEMENTO (COD_QUEIM, CAMPO, VALOR) VALUES (?, ?, ?)\"\n",
    "        values_complemento_fogo = (id, 'Riscofogo', fogo)\n",
    "        \n",
    "        cursor.execute(sql_insert_queimada, values_queimada)\n",
    "        cursor.execute(sql_insert_queimada_complemento_bioma, values_complemento_bioma)\n",
    "        cursor.execute(sql_insert_queimada_complemento_chuva, values_complemento_chuva)\n",
    "        cursor.execute(sql_insert_queimada_complemento_precipitacao, values_complemento_precipitacao)\n",
    "        cursor.execute(sql_insert_queimada_complemento_fogo, values_complemento_fogo)\n",
    "    \n",
    "        connection.commit()\n",
    "             \n",
    "    except Exception as e:\n",
    "        print(f'Error in connection: {e}')\n",
    "        \n",
    "    finally:\n",
    "        if 'cursor' in locals():\n",
    "            cursor.close()\n",
    "            \n",
    "        if 'connection' in locals():\n",
    "            connection.close()\n",
    "            \n",
    "    return id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import ftfy\n",
    "import codecs\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "ano = \"2023\"\n",
    "mes = \"10\"\n",
    "\n",
    "url = f'https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/mensal/Brasil/focos_mensal_br_{ano}{mes}.csv'\n",
    "\n",
    "arquivo_csv  = 'arquivo.csv'\n",
    "arquivo_text = 'insert.txt'\n",
    "arquivo_json = 'arquivo.json'\n",
    "arquivo_text_comp = 'insert_comp.txt'\n",
    "\n",
    "dados_csv_np         = np.array([])\n",
    "\n",
    "insert_queimada      = []\n",
    "insert_queimada_comp = []\n",
    "\n",
    "isRange = False\n",
    "foreign_key = 0\n",
    "\n",
    "data_hora_ini_str = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}', '2023-10-21 00:00:00')\n",
    "data_hora_fim_str = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}', '2023-10-23 23:59:59')\n",
    "\n",
    "data_hora_ini = datetime.strptime(data_hora_ini_str[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "data_hora_fim = datetime.strptime(data_hora_fim_str[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "rangeTeste = 0\n",
    "\n",
    "with requests.get(url, stream=True, verify=False) as response:\n",
    "    if response.status_code == 200:\n",
    "        dados_csv = response.iter_lines(decode_unicode=True)\n",
    "        \n",
    "        csv_reader = csv.DictReader(dados_csv)        \n",
    "        for row in csv_reader:\n",
    "            \n",
    "            data_hora_csv_str = re.search(r'[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}', row['data_hora_gmt'])\n",
    "            data_hora_csv = datetime.strptime(data_hora_csv_str[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            if(data_hora_ini <= data_hora_csv <= data_hora_fim):\n",
    "                \n",
    "                foreign_key = insertTable(row)                                        # database\n",
    "                createInsert(row, insert_queimada, insert_queimada_comp, foreign_key) # script\n",
    "                dados_csv_np = np.append(dados_csv_np, row)                           # json\n",
    "\n",
    "                isRange = True\n",
    "            if (data_hora_csv > data_hora_fim):\n",
    "                break\n",
    "                \n",
    "           # breakpoint = filterData(row, dados_csv, data_hora_ini, data_hora_fim, insert_queimada, insert_queimada_comp, isRange)\n",
    "           # if(breakpoint == 0):\n",
    "           #     break\n",
    "\n",
    "        \n",
    "if isRange:\n",
    "    novo_dados = dados_csv_np[1:].tolist()\n",
    "    novo_dados_csv = json.dumps(novo_dados, ensure_ascii=False, indent=4)\n",
    "    novo_dados_json = ftfy.fix_text(novo_dados_csv)\n",
    "    with codecs.open(arquivo_json, 'w', encoding='utf-8') as arquivo:\n",
    "        arquivo.write(novo_dados_json)\n",
    "    \n",
    "    with codecs.open(arquivo_text, 'w', encoding='utf-8') as arquivo:\n",
    "        for linha in insert_queimada:\n",
    "            arquivo.write(f\"{ftfy.fix_text(linha)}\")\n",
    "            \n",
    "    with codecs.open(arquivo_text_comp, 'w', encoding='utf-8') as arquivo:\n",
    "        for linha in insert_queimada_comp:\n",
    "            arquivo.write(f\"{ftfy.fix_text(linha)}\")\n",
    "    \n",
    "    print(f'Conversão concluída. Os dados JSON foram salvos em {arquivo_json}')\n",
    "    print(f'Conversão concluída. Os dados TXT foram salvos em {arquivo_text}')\n",
    "    print(f'Conversão concluída. Os dados TXT foram salvos em {arquivo_text_comp}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
